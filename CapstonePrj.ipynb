{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeaFyFbHC6u6",
        "outputId": "9f67d389-dab5-49db-ef30-a5b59b806645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for saving checkpoints and precomputed data\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/checkpoints'\n",
        "PRECOMPUTE_DIR = '/content/drive/MyDrive/precomputed_spectrograms'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(PRECOMPUTE_DIR, exist_ok=True)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_audio(file_path, sr=16000, top_db=30, duration=5, n_mels=128, n_fft=1024, hop_length=256):\n",
        "    \"\"\"\n",
        "    Preprocess audio: load, trim, pad/truncate, normalize, convert to mel spectrogram.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to audio file\n",
        "        sr (int): Sampling rate (16000 Hz)\n",
        "        top_db (int): Trimming threshold (30 dB)\n",
        "        duration (int): Target duration (5 seconds)\n",
        "        n_mels (int): Number of mel bins\n",
        "        n_fft (int): FFT window size\n",
        "        hop_length (int): Hop length for STFT\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Mel spectrogram in dB, normalized to [0,1]\n",
        "    \"\"\"\n",
        "    y, _ = librosa.load(file_path, sr=sr)\n",
        "    y, _ = librosa.effects.trim(y, top_db=top_db)\n",
        "    target_length = sr * duration\n",
        "    if len(y) < target_length:\n",
        "        y = np.pad(y, (0, target_length - len(y)))\n",
        "    else:\n",
        "        y = y[:target_length]\n",
        "    y = y / np.max(np.abs(y)) if np.max(np.abs(y)) != 0 else y  # Normalize to [-1,1]\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    S_dB = (S_dB - np.min(S_dB)) / (np.max(S_dB) - np.min(S_dB))  # Normalize to [0,1]\n",
        "    return S_dB\n",
        "\n",
        "# Function to precompute spectrograms\n",
        "def precompute_spectrograms(clean_dir, noisy_dir, save_dir):\n",
        "    clean_files = sorted(os.listdir(clean_dir))\n",
        "    noisy_files = sorted(os.listdir(noisy_dir))\n",
        "    for i, (clean_file, noisy_file) in enumerate(zip(clean_files, noisy_files)):\n",
        "        clean_mel = preprocess_audio(os.path.join(clean_dir, clean_file))\n",
        "        noisy_mel = preprocess_audio(os.path.join(noisy_dir, noisy_file))\n",
        "        np.save(os.path.join(save_dir, f'clean_{i}.npy'), clean_mel)\n",
        "        np.save(os.path.join(save_dir, f'noisy_{i}.npy'), noisy_mel)\n",
        "    print(f\"Precomputed spectrograms saved to {save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XTb34pp0Eiy6"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class VoiceBankDataset(Dataset):\n",
        "    def __init__(self, clean_dir, noisy_dir, precompute_dir=None):\n",
        "        \"\"\"\n",
        "        Dataset for VoiceBank clean and noisy audio pairs.\n",
        "        If precompute_dir is provided, loads precomputed spectrograms.\n",
        "        \"\"\"\n",
        "        self.clean_files = sorted(os.listdir(clean_dir))\n",
        "        self.noisy_files = sorted(os.listdir(noisy_dir))\n",
        "        self.clean_dir = clean_dir\n",
        "        self.noisy_dir = noisy_dir\n",
        "        self.precompute_dir = precompute_dir\n",
        "        if precompute_dir:\n",
        "            self.clean_mels = [os.path.join(precompute_dir, f'clean_{i}.npy') for i in range(len(self.clean_files))]\n",
        "            self.noisy_mels = [os.path.join(precompute_dir, f'noisy_{i}.npy') for i in range(len(self.noisy_files))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.precompute_dir:\n",
        "            clean_mel = np.load(self.clean_mels[idx])\n",
        "            noisy_mel = np.load(self.noisy_mels[idx])\n",
        "        else:\n",
        "            clean_mel = preprocess_audio(os.path.join(self.clean_dir, self.clean_files[idx]))\n",
        "            noisy_mel = preprocess_audio(os.path.join(self.noisy_dir, self.noisy_files[idx]))\n",
        "        return torch.tensor(noisy_mel, dtype=torch.float32), torch.tensor(clean_mel, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK0Y5SuQEonB"
      },
      "outputs": [],
      "source": [
        "# Generator model (BLSTM + CNN)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_mels=128, time_frames=313):\n",
        "        super(Generator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.blstm = nn.LSTM(32 * n_mels, 256, bidirectional=True, batch_first=True)\n",
        "        self.dense = nn.Linear(512, n_mels)  # 512 = 256 * 2 (bidirectional)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.conv2(x), 0.2)\n",
        "        batch, channels, height, time = x.shape\n",
        "        x = x.permute(0, 3, 2, 1).reshape(batch, time, channels * height)\n",
        "        x, _ = self.blstm(x)\n",
        "        x = self.dense(x)\n",
        "        x = x.permute(0, 2, 1).unsqueeze(1)\n",
        "        return torch.sigmoid(x)  # output is in [0,1]\n",
        "\n",
        "# Discriminator model (CNN)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(512 * 8 * 19, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = nn.functional.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = self.flatten(x)\n",
        "        return torch.sigmoid(self.dense(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sWS7MbGQEwPX"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(generator, discriminator, train_loader, num_epochs, device, lambda_l1=100):\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Reduced LR for D\n",
        "    criterion_bce = nn.BCELoss()\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_epoch = 0\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, 'latest_checkpoint.pth')\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        for batch_idx, (noisy_mel, clean_mel) in enumerate(train_loader):\n",
        "            noisy_mel = noisy_mel.unsqueeze(1).to(device)  # Add channel dim: [batch, 1, 128, 313]\n",
        "            clean_mel = clean_mel.unsqueeze(1).to(device)\n",
        "            batch_size = noisy_mel.size(0)\n",
        "\n",
        "            # Labels\n",
        "            real_labels = torch.full((batch_size, 1), 0.9, device=device)  # Label smoothing\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            # ---------------------\n",
        "            # Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            output_real = discriminator(clean_mel)\n",
        "            loss_D_real = criterion_bce(output_real, real_labels)\n",
        "\n",
        "            enhanced_mel = generator(noisy_mel)\n",
        "            output_fake = discriminator(enhanced_mel.detach())\n",
        "            loss_D_fake = criterion_bce(output_fake, fake_labels)\n",
        "\n",
        "            loss_D = loss_D_real + loss_D_fake\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # -----------------\n",
        "            # Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "            output_fake = discriminator(enhanced_mel)\n",
        "            loss_G_bce = criterion_bce(output_fake, real_labels)  # Fool discriminator\n",
        "            loss_G_l1 = criterion_l1(enhanced_mel, clean_mel)     # Reconstruction loss\n",
        "            loss_G = loss_G_bce + lambda_l1 * loss_G_l1\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{num_epochs-1}, Batch {batch_idx}/{len(train_loader)-1}, \"\n",
        "                      f\"Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FikWURExZ_M8",
        "outputId": "c5545d4e-c7be-44df-e473-6754b72bfe5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Resuming training from epoch 1\n",
            "Epoch 1/49, Batch 0/723, Loss D: 0.5114, Loss G: 12.1968\n",
            "Epoch 1/49, Batch 100/723, Loss D: 0.7939, Loss G: 10.6354\n",
            "Epoch 1/49, Batch 200/723, Loss D: 0.8932, Loss G: 12.2324\n",
            "Epoch 1/49, Batch 300/723, Loss D: 0.9438, Loss G: 12.5707\n",
            "Epoch 1/49, Batch 400/723, Loss D: 0.7437, Loss G: 11.6717\n",
            "Epoch 1/49, Batch 500/723, Loss D: 1.3386, Loss G: 11.2023\n",
            "Epoch 1/49, Batch 600/723, Loss D: 0.9567, Loss G: 11.6519\n",
            "Epoch 1/49, Batch 700/723, Loss D: 0.8060, Loss G: 11.2174\n",
            "Checkpoint saved at epoch 1\n",
            "Epoch 2/49, Batch 0/723, Loss D: 1.0915, Loss G: 9.6956\n",
            "Epoch 2/49, Batch 100/723, Loss D: 0.8416, Loss G: 10.0197\n",
            "Epoch 2/49, Batch 200/723, Loss D: 0.6451, Loss G: 10.2824\n",
            "Epoch 2/49, Batch 300/723, Loss D: 0.9079, Loss G: 10.0525\n",
            "Epoch 2/49, Batch 400/723, Loss D: 0.8713, Loss G: 10.4867\n",
            "Epoch 2/49, Batch 500/723, Loss D: 0.8588, Loss G: 10.7408\n",
            "Epoch 2/49, Batch 600/723, Loss D: 0.8769, Loss G: 10.2894\n",
            "Epoch 2/49, Batch 700/723, Loss D: 0.7795, Loss G: 10.5619\n",
            "Checkpoint saved at epoch 2\n",
            "Epoch 3/49, Batch 0/723, Loss D: 0.8594, Loss G: 9.1110\n",
            "Epoch 3/49, Batch 100/723, Loss D: 0.9111, Loss G: 9.7707\n",
            "Epoch 3/49, Batch 200/723, Loss D: 1.3070, Loss G: 9.4258\n",
            "Epoch 3/49, Batch 300/723, Loss D: 0.8744, Loss G: 10.2854\n",
            "Epoch 3/49, Batch 400/723, Loss D: 0.3618, Loss G: 15.3239\n",
            "Epoch 3/49, Batch 500/723, Loss D: 0.6885, Loss G: 12.7968\n",
            "Epoch 3/49, Batch 600/723, Loss D: 0.8007, Loss G: 11.2426\n",
            "Epoch 3/49, Batch 700/723, Loss D: 0.6479, Loss G: 12.0227\n",
            "Checkpoint saved at epoch 3\n",
            "Epoch 4/49, Batch 0/723, Loss D: 2.0269, Loss G: 12.5982\n",
            "Epoch 4/49, Batch 100/723, Loss D: 0.8188, Loss G: 10.2681\n",
            "Epoch 4/49, Batch 200/723, Loss D: 1.2594, Loss G: 11.5513\n",
            "Epoch 4/49, Batch 300/723, Loss D: 0.9300, Loss G: 10.1644\n",
            "Epoch 4/49, Batch 400/723, Loss D: 0.7689, Loss G: 11.8474\n",
            "Epoch 4/49, Batch 500/723, Loss D: 0.7737, Loss G: 12.3582\n",
            "Epoch 4/49, Batch 600/723, Loss D: 1.1441, Loss G: 8.2902\n",
            "Epoch 4/49, Batch 700/723, Loss D: 0.9500, Loss G: 9.1202\n",
            "Checkpoint saved at epoch 4\n",
            "Epoch 5/49, Batch 0/723, Loss D: 0.6542, Loss G: 11.6484\n",
            "Epoch 5/49, Batch 100/723, Loss D: 0.6892, Loss G: 10.5174\n",
            "Epoch 5/49, Batch 200/723, Loss D: 0.7498, Loss G: 10.2310\n",
            "Epoch 5/49, Batch 300/723, Loss D: 0.9309, Loss G: 9.2153\n",
            "Epoch 5/49, Batch 400/723, Loss D: 0.9947, Loss G: 10.1217\n",
            "Epoch 5/49, Batch 500/723, Loss D: 0.8877, Loss G: 10.6445\n",
            "Epoch 5/49, Batch 600/723, Loss D: 0.9458, Loss G: 9.4720\n",
            "Epoch 5/49, Batch 700/723, Loss D: 0.8731, Loss G: 10.1699\n",
            "Checkpoint saved at epoch 5\n",
            "Epoch 6/49, Batch 0/723, Loss D: 0.8842, Loss G: 11.5532\n",
            "Epoch 6/49, Batch 100/723, Loss D: 0.9183, Loss G: 9.4135\n",
            "Epoch 6/49, Batch 200/723, Loss D: 0.9233, Loss G: 10.1182\n",
            "Epoch 6/49, Batch 300/723, Loss D: 0.8988, Loss G: 9.3617\n",
            "Epoch 6/49, Batch 400/723, Loss D: 0.8167, Loss G: 10.3314\n",
            "Epoch 6/49, Batch 500/723, Loss D: 0.8818, Loss G: 10.9242\n",
            "Epoch 6/49, Batch 600/723, Loss D: 0.7964, Loss G: 8.6480\n",
            "Epoch 6/49, Batch 700/723, Loss D: 1.1006, Loss G: 11.0452\n",
            "Checkpoint saved at epoch 6\n",
            "Epoch 7/49, Batch 0/723, Loss D: 0.9935, Loss G: 9.2829\n",
            "Epoch 7/49, Batch 100/723, Loss D: 0.9087, Loss G: 13.4553\n",
            "Epoch 7/49, Batch 200/723, Loss D: 0.9354, Loss G: 9.9439\n",
            "Epoch 7/49, Batch 300/723, Loss D: 0.7363, Loss G: 10.9447\n",
            "Epoch 7/49, Batch 400/723, Loss D: 0.7073, Loss G: 10.6607\n",
            "Epoch 7/49, Batch 500/723, Loss D: 0.8750, Loss G: 12.8610\n",
            "Epoch 7/49, Batch 600/723, Loss D: 0.8181, Loss G: 9.5308\n",
            "Epoch 7/49, Batch 700/723, Loss D: 0.8142, Loss G: 10.5701\n",
            "Checkpoint saved at epoch 7\n",
            "Epoch 8/49, Batch 0/723, Loss D: 0.7688, Loss G: 8.7883\n",
            "Epoch 8/49, Batch 100/723, Loss D: 0.8358, Loss G: 10.8239\n",
            "Epoch 8/49, Batch 200/723, Loss D: 0.7911, Loss G: 9.3271\n",
            "Epoch 8/49, Batch 300/723, Loss D: 0.8352, Loss G: 10.1521\n",
            "Epoch 8/49, Batch 400/723, Loss D: 0.8148, Loss G: 10.3799\n",
            "Epoch 8/49, Batch 500/723, Loss D: 0.6531, Loss G: 10.6857\n",
            "Epoch 8/49, Batch 600/723, Loss D: 0.8024, Loss G: 9.8103\n",
            "Epoch 8/49, Batch 700/723, Loss D: 0.6921, Loss G: 9.9918\n",
            "Checkpoint saved at epoch 8\n",
            "Epoch 9/49, Batch 0/723, Loss D: 0.8590, Loss G: 11.2182\n",
            "Epoch 9/49, Batch 100/723, Loss D: 1.8224, Loss G: 10.9586\n",
            "Epoch 9/49, Batch 200/723, Loss D: 0.8866, Loss G: 10.5877\n",
            "Epoch 9/49, Batch 300/723, Loss D: 0.7785, Loss G: 10.4119\n",
            "Epoch 9/49, Batch 400/723, Loss D: 0.7162, Loss G: 8.4871\n",
            "Epoch 9/49, Batch 500/723, Loss D: 0.5932, Loss G: 10.7813\n",
            "Epoch 9/49, Batch 600/723, Loss D: 0.9005, Loss G: 11.1180\n",
            "Epoch 9/49, Batch 700/723, Loss D: 0.5498, Loss G: 12.2599\n",
            "Checkpoint saved at epoch 9\n",
            "Epoch 10/49, Batch 0/723, Loss D: 1.1020, Loss G: 11.0806\n",
            "Epoch 10/49, Batch 100/723, Loss D: 0.7527, Loss G: 10.6418\n",
            "Epoch 10/49, Batch 200/723, Loss D: 0.9129, Loss G: 9.0609\n",
            "Epoch 10/49, Batch 300/723, Loss D: 0.8040, Loss G: 10.1261\n",
            "Epoch 10/49, Batch 400/723, Loss D: 0.7050, Loss G: 11.5417\n",
            "Epoch 10/49, Batch 500/723, Loss D: 0.6987, Loss G: 9.7235\n",
            "Epoch 10/49, Batch 600/723, Loss D: 0.7550, Loss G: 9.7822\n",
            "Epoch 10/49, Batch 700/723, Loss D: 0.7428, Loss G: 10.2536\n",
            "Checkpoint saved at epoch 10\n",
            "Epoch 11/49, Batch 0/723, Loss D: 0.8026, Loss G: 10.4214\n",
            "Epoch 11/49, Batch 100/723, Loss D: 0.9692, Loss G: 9.4010\n",
            "Epoch 11/49, Batch 200/723, Loss D: 0.7168, Loss G: 8.9730\n",
            "Epoch 11/49, Batch 300/723, Loss D: 0.8706, Loss G: 8.6688\n",
            "Epoch 11/49, Batch 400/723, Loss D: 0.6791, Loss G: 11.5376\n",
            "Epoch 11/49, Batch 500/723, Loss D: 0.9136, Loss G: 10.0947\n",
            "Epoch 11/49, Batch 600/723, Loss D: 0.9860, Loss G: 10.7781\n",
            "Epoch 11/49, Batch 700/723, Loss D: 0.8374, Loss G: 9.3512\n",
            "Checkpoint saved at epoch 11\n",
            "Epoch 12/49, Batch 0/723, Loss D: 0.7441, Loss G: 11.5652\n",
            "Epoch 12/49, Batch 100/723, Loss D: 0.7088, Loss G: 9.3932\n",
            "Epoch 12/49, Batch 200/723, Loss D: 0.8209, Loss G: 9.6345\n",
            "Epoch 12/49, Batch 300/723, Loss D: 0.8802, Loss G: 10.6650\n",
            "Epoch 12/49, Batch 400/723, Loss D: 0.6514, Loss G: 11.6088\n",
            "Epoch 12/49, Batch 500/723, Loss D: 0.7095, Loss G: 9.1154\n",
            "Epoch 12/49, Batch 600/723, Loss D: 0.7698, Loss G: 9.9765\n",
            "Epoch 12/49, Batch 700/723, Loss D: 0.7802, Loss G: 7.4762\n",
            "Checkpoint saved at epoch 12\n",
            "Epoch 13/49, Batch 0/723, Loss D: 0.7523, Loss G: 10.7890\n",
            "Epoch 13/49, Batch 100/723, Loss D: 0.8354, Loss G: 9.3847\n",
            "Epoch 13/49, Batch 200/723, Loss D: 0.6807, Loss G: 11.0373\n",
            "Epoch 13/49, Batch 300/723, Loss D: 0.8558, Loss G: 12.3026\n",
            "Epoch 13/49, Batch 400/723, Loss D: 0.7767, Loss G: 10.9044\n",
            "Epoch 13/49, Batch 500/723, Loss D: 0.6070, Loss G: 12.8057\n",
            "Epoch 13/49, Batch 600/723, Loss D: 0.6376, Loss G: 11.7124\n",
            "Epoch 13/49, Batch 700/723, Loss D: 0.6007, Loss G: 9.5892\n",
            "Checkpoint saved at epoch 13\n",
            "Epoch 14/49, Batch 0/723, Loss D: 1.4133, Loss G: 12.6680\n",
            "Epoch 14/49, Batch 100/723, Loss D: 0.7951, Loss G: 10.4674\n",
            "Epoch 14/49, Batch 200/723, Loss D: 0.9823, Loss G: 11.3403\n",
            "Epoch 14/49, Batch 300/723, Loss D: 0.7722, Loss G: 10.0873\n",
            "Epoch 14/49, Batch 400/723, Loss D: 0.9509, Loss G: 8.6573\n",
            "Epoch 14/49, Batch 500/723, Loss D: 0.8823, Loss G: 10.9235\n",
            "Epoch 14/49, Batch 600/723, Loss D: 1.0472, Loss G: 11.1091\n",
            "Epoch 14/49, Batch 700/723, Loss D: 0.5788, Loss G: 9.8423\n",
            "Checkpoint saved at epoch 14\n",
            "Epoch 15/49, Batch 0/723, Loss D: 0.8035, Loss G: 10.0462\n",
            "Epoch 15/49, Batch 100/723, Loss D: 0.7476, Loss G: 11.2836\n",
            "Epoch 15/49, Batch 200/723, Loss D: 0.7007, Loss G: 9.9328\n",
            "Epoch 15/49, Batch 300/723, Loss D: 0.6186, Loss G: 10.5975\n",
            "Epoch 15/49, Batch 400/723, Loss D: 0.7169, Loss G: 10.6442\n",
            "Epoch 15/49, Batch 500/723, Loss D: 0.6063, Loss G: 12.4268\n",
            "Epoch 15/49, Batch 600/723, Loss D: 0.7178, Loss G: 9.5801\n",
            "Epoch 15/49, Batch 700/723, Loss D: 0.6380, Loss G: 10.5418\n",
            "Checkpoint saved at epoch 15\n",
            "Epoch 16/49, Batch 0/723, Loss D: 0.8376, Loss G: 9.5259\n",
            "Epoch 16/49, Batch 100/723, Loss D: 0.7133, Loss G: 9.1611\n",
            "Epoch 16/49, Batch 200/723, Loss D: 0.5856, Loss G: 11.8872\n",
            "Epoch 16/49, Batch 300/723, Loss D: 0.6363, Loss G: 11.8977\n",
            "Epoch 16/49, Batch 400/723, Loss D: 0.7513, Loss G: 10.5545\n",
            "Epoch 16/49, Batch 500/723, Loss D: 0.6450, Loss G: 9.3205\n",
            "Epoch 16/49, Batch 600/723, Loss D: 0.6102, Loss G: 10.8332\n",
            "Epoch 16/49, Batch 700/723, Loss D: 0.6961, Loss G: 10.2657\n",
            "Checkpoint saved at epoch 16\n",
            "Epoch 17/49, Batch 0/723, Loss D: 0.6195, Loss G: 11.2547\n",
            "Epoch 17/49, Batch 100/723, Loss D: 0.5886, Loss G: 9.4973\n",
            "Epoch 17/49, Batch 200/723, Loss D: 0.7564, Loss G: 9.3987\n",
            "Epoch 17/49, Batch 300/723, Loss D: 0.7990, Loss G: 9.9668\n",
            "Epoch 17/49, Batch 400/723, Loss D: 0.6985, Loss G: 12.5081\n",
            "Epoch 17/49, Batch 500/723, Loss D: 0.8408, Loss G: 9.6038\n",
            "Epoch 17/49, Batch 600/723, Loss D: 0.6901, Loss G: 10.6261\n",
            "Epoch 17/49, Batch 700/723, Loss D: 0.6934, Loss G: 12.1594\n",
            "Checkpoint saved at epoch 17\n",
            "Epoch 18/49, Batch 0/723, Loss D: 0.6912, Loss G: 11.5106\n",
            "Epoch 18/49, Batch 100/723, Loss D: 0.7271, Loss G: 8.9681\n",
            "Epoch 18/49, Batch 200/723, Loss D: 0.7136, Loss G: 11.3536\n",
            "Epoch 18/49, Batch 300/723, Loss D: 0.6505, Loss G: 10.5984\n",
            "Epoch 18/49, Batch 400/723, Loss D: 0.8258, Loss G: 8.5764\n",
            "Epoch 18/49, Batch 500/723, Loss D: 0.5867, Loss G: 11.0020\n",
            "Epoch 18/49, Batch 600/723, Loss D: 0.3636, Loss G: 20.8915\n",
            "Epoch 18/49, Batch 700/723, Loss D: 0.3439, Loss G: 22.5359\n",
            "Checkpoint saved at epoch 18\n",
            "Epoch 19/49, Batch 0/723, Loss D: 0.3440, Loss G: 20.3574\n",
            "Epoch 19/49, Batch 100/723, Loss D: 0.3471, Loss G: 20.2356\n",
            "Epoch 19/49, Batch 200/723, Loss D: 0.3379, Loss G: 18.5438\n",
            "Epoch 19/49, Batch 300/723, Loss D: 0.3379, Loss G: 21.3336\n",
            "Epoch 19/49, Batch 400/723, Loss D: 0.3384, Loss G: 22.4564\n",
            "Epoch 19/49, Batch 500/723, Loss D: 0.3412, Loss G: 19.6609\n",
            "Epoch 19/49, Batch 600/723, Loss D: 0.3333, Loss G: 22.5865\n",
            "Epoch 19/49, Batch 700/723, Loss D: 0.3415, Loss G: 21.7151\n",
            "Checkpoint saved at epoch 19\n",
            "Epoch 20/49, Batch 0/723, Loss D: 0.3364, Loss G: 21.5626\n",
            "Epoch 20/49, Batch 100/723, Loss D: 0.3387, Loss G: 22.0534\n",
            "Epoch 20/49, Batch 200/723, Loss D: 0.3627, Loss G: 16.1051\n",
            "Epoch 20/49, Batch 300/723, Loss D: 0.4244, Loss G: 15.2329\n",
            "Epoch 20/49, Batch 400/723, Loss D: 0.5082, Loss G: 14.3756\n",
            "Epoch 20/49, Batch 500/723, Loss D: 0.6049, Loss G: 9.2032\n",
            "Epoch 20/49, Batch 600/723, Loss D: 0.4422, Loss G: 13.5420\n",
            "Epoch 20/49, Batch 700/723, Loss D: 0.4533, Loss G: 11.4380\n",
            "Checkpoint saved at epoch 20\n",
            "Epoch 21/49, Batch 0/723, Loss D: 0.5479, Loss G: 9.9070\n",
            "Epoch 21/49, Batch 100/723, Loss D: 0.6361, Loss G: 10.4838\n",
            "Epoch 21/49, Batch 200/723, Loss D: 0.4468, Loss G: 11.3312\n",
            "Epoch 21/49, Batch 300/723, Loss D: 0.5466, Loss G: 10.9917\n",
            "Epoch 21/49, Batch 400/723, Loss D: 0.5053, Loss G: 9.8006\n",
            "Epoch 21/49, Batch 500/723, Loss D: 0.6814, Loss G: 11.2414\n",
            "Epoch 21/49, Batch 600/723, Loss D: 0.7270, Loss G: 12.3855\n",
            "Epoch 21/49, Batch 700/723, Loss D: 0.4589, Loss G: 14.3549\n",
            "Checkpoint saved at epoch 21\n",
            "Epoch 22/49, Batch 0/723, Loss D: 0.6331, Loss G: 12.9648\n",
            "Epoch 22/49, Batch 100/723, Loss D: 0.5468, Loss G: 13.9597\n",
            "Epoch 22/49, Batch 200/723, Loss D: 0.4722, Loss G: 11.4056\n",
            "Epoch 22/49, Batch 300/723, Loss D: 0.5640, Loss G: 9.8085\n",
            "Epoch 22/49, Batch 400/723, Loss D: 0.6612, Loss G: 11.4649\n",
            "Epoch 22/49, Batch 500/723, Loss D: 0.4604, Loss G: 13.3132\n",
            "Epoch 22/49, Batch 600/723, Loss D: 0.5348, Loss G: 10.6696\n",
            "Epoch 22/49, Batch 700/723, Loss D: 0.5422, Loss G: 11.2988\n",
            "Checkpoint saved at epoch 22\n",
            "Epoch 23/49, Batch 0/723, Loss D: 0.5174, Loss G: 9.8402\n",
            "Epoch 23/49, Batch 100/723, Loss D: 0.4251, Loss G: 13.3678\n",
            "Epoch 23/49, Batch 200/723, Loss D: 0.4763, Loss G: 12.7872\n",
            "Epoch 23/49, Batch 300/723, Loss D: 0.7021, Loss G: 9.7256\n",
            "Epoch 23/49, Batch 400/723, Loss D: 0.5811, Loss G: 12.0288\n",
            "Epoch 23/49, Batch 500/723, Loss D: 0.5295, Loss G: 10.2850\n",
            "Epoch 23/49, Batch 600/723, Loss D: 0.5583, Loss G: 11.4066\n",
            "Epoch 23/49, Batch 700/723, Loss D: 0.5152, Loss G: 11.7693\n",
            "Checkpoint saved at epoch 23\n",
            "Epoch 24/49, Batch 0/723, Loss D: 0.4963, Loss G: 13.1523\n",
            "Epoch 24/49, Batch 100/723, Loss D: 0.5486, Loss G: 12.0852\n",
            "Epoch 24/49, Batch 200/723, Loss D: 0.5995, Loss G: 11.3248\n",
            "Epoch 24/49, Batch 300/723, Loss D: 0.5815, Loss G: 13.0331\n",
            "Epoch 24/49, Batch 400/723, Loss D: 0.6388, Loss G: 11.3522\n",
            "Epoch 24/49, Batch 500/723, Loss D: 0.5791, Loss G: 9.9881\n",
            "Epoch 24/49, Batch 600/723, Loss D: 0.6771, Loss G: 11.0418\n",
            "Epoch 24/49, Batch 700/723, Loss D: 0.4916, Loss G: 12.8291\n",
            "Checkpoint saved at epoch 24\n",
            "Epoch 25/49, Batch 0/723, Loss D: 0.6571, Loss G: 10.0776\n",
            "Epoch 25/49, Batch 100/723, Loss D: 0.4514, Loss G: 11.8958\n",
            "Epoch 25/49, Batch 200/723, Loss D: 0.4900, Loss G: 10.7116\n",
            "Epoch 25/49, Batch 300/723, Loss D: 0.4462, Loss G: 13.7016\n",
            "Epoch 25/49, Batch 400/723, Loss D: 0.4548, Loss G: 13.1748\n",
            "Epoch 25/49, Batch 500/723, Loss D: 0.5355, Loss G: 10.2452\n",
            "Epoch 25/49, Batch 600/723, Loss D: 0.4837, Loss G: 14.0412\n",
            "Epoch 25/49, Batch 700/723, Loss D: 0.5431, Loss G: 11.5378\n",
            "Checkpoint saved at epoch 25\n",
            "Epoch 26/49, Batch 0/723, Loss D: 0.4924, Loss G: 11.2023\n",
            "Epoch 26/49, Batch 100/723, Loss D: 0.4969, Loss G: 12.2904\n",
            "Epoch 26/49, Batch 200/723, Loss D: 0.5949, Loss G: 10.1553\n",
            "Epoch 26/49, Batch 300/723, Loss D: 0.4765, Loss G: 11.9233\n",
            "Epoch 26/49, Batch 400/723, Loss D: 0.5048, Loss G: 11.5082\n",
            "Epoch 26/49, Batch 500/723, Loss D: 0.6650, Loss G: 10.5153\n",
            "Epoch 26/49, Batch 600/723, Loss D: 0.6935, Loss G: 13.0529\n",
            "Epoch 26/49, Batch 700/723, Loss D: 0.6812, Loss G: 10.3832\n",
            "Checkpoint saved at epoch 26\n",
            "Epoch 27/49, Batch 0/723, Loss D: 0.5922, Loss G: 13.0661\n",
            "Epoch 27/49, Batch 100/723, Loss D: 0.4974, Loss G: 10.3017\n",
            "Epoch 27/49, Batch 200/723, Loss D: 0.4600, Loss G: 11.8638\n",
            "Epoch 27/49, Batch 300/723, Loss D: 0.7199, Loss G: 12.9341\n",
            "Epoch 27/49, Batch 400/723, Loss D: 0.5794, Loss G: 11.6198\n",
            "Epoch 27/49, Batch 500/723, Loss D: 0.5701, Loss G: 9.9084\n",
            "Epoch 27/49, Batch 600/723, Loss D: 0.5519, Loss G: 12.1946\n",
            "Epoch 27/49, Batch 700/723, Loss D: 0.5796, Loss G: 10.0846\n",
            "Checkpoint saved at epoch 27\n",
            "Epoch 28/49, Batch 0/723, Loss D: 0.5136, Loss G: 10.8364\n",
            "Epoch 28/49, Batch 100/723, Loss D: 0.8303, Loss G: 11.0990\n",
            "Epoch 28/49, Batch 200/723, Loss D: 0.4644, Loss G: 13.1869\n",
            "Epoch 28/49, Batch 300/723, Loss D: 0.6468, Loss G: 12.1353\n",
            "Epoch 28/49, Batch 400/723, Loss D: 0.6041, Loss G: 12.6197\n",
            "Epoch 28/49, Batch 500/723, Loss D: 0.5686, Loss G: 13.1024\n",
            "Epoch 28/49, Batch 600/723, Loss D: 0.5819, Loss G: 11.0326\n",
            "Epoch 28/49, Batch 700/723, Loss D: 0.6770, Loss G: 10.6056\n",
            "Checkpoint saved at epoch 28\n",
            "Epoch 29/49, Batch 0/723, Loss D: 0.5450, Loss G: 11.2586\n",
            "Epoch 29/49, Batch 100/723, Loss D: 0.4341, Loss G: 14.2999\n",
            "Epoch 29/49, Batch 200/723, Loss D: 0.4292, Loss G: 11.0004\n",
            "Epoch 29/49, Batch 300/723, Loss D: 0.5178, Loss G: 13.0383\n",
            "Epoch 29/49, Batch 400/723, Loss D: 0.7362, Loss G: 12.7257\n",
            "Epoch 29/49, Batch 500/723, Loss D: 0.4703, Loss G: 12.8424\n",
            "Epoch 29/49, Batch 600/723, Loss D: 0.5919, Loss G: 12.2182\n",
            "Epoch 29/49, Batch 700/723, Loss D: 0.5252, Loss G: 11.2857\n",
            "Checkpoint saved at epoch 29\n",
            "Epoch 30/49, Batch 0/723, Loss D: 0.7066, Loss G: 14.3196\n",
            "Epoch 30/49, Batch 100/723, Loss D: 0.4475, Loss G: 11.5903\n",
            "Epoch 30/49, Batch 200/723, Loss D: 0.4681, Loss G: 12.8415\n",
            "Epoch 30/49, Batch 300/723, Loss D: 0.4882, Loss G: 12.5441\n",
            "Epoch 30/49, Batch 400/723, Loss D: 0.5630, Loss G: 11.7060\n",
            "Epoch 30/49, Batch 500/723, Loss D: 0.5489, Loss G: 13.6289\n",
            "Epoch 30/49, Batch 600/723, Loss D: 0.4981, Loss G: 9.7714\n",
            "Epoch 30/49, Batch 700/723, Loss D: 0.5292, Loss G: 12.2068\n",
            "Checkpoint saved at epoch 30\n",
            "Epoch 31/49, Batch 0/723, Loss D: 0.5362, Loss G: 12.5262\n",
            "Epoch 31/49, Batch 100/723, Loss D: 0.5249, Loss G: 11.4505\n",
            "Epoch 31/49, Batch 200/723, Loss D: 0.5114, Loss G: 11.9297\n",
            "Epoch 31/49, Batch 300/723, Loss D: 0.5993, Loss G: 11.7135\n",
            "Epoch 31/49, Batch 400/723, Loss D: 0.7891, Loss G: 11.3376\n",
            "Epoch 31/49, Batch 500/723, Loss D: 0.5027, Loss G: 10.9001\n",
            "Epoch 31/49, Batch 600/723, Loss D: 0.5403, Loss G: 11.1860\n",
            "Epoch 31/49, Batch 700/723, Loss D: 0.5180, Loss G: 9.7644\n",
            "Checkpoint saved at epoch 31\n",
            "Epoch 32/49, Batch 0/723, Loss D: 0.4851, Loss G: 11.8142\n",
            "Epoch 32/49, Batch 100/723, Loss D: 0.4887, Loss G: 12.6186\n",
            "Epoch 32/49, Batch 200/723, Loss D: 0.4839, Loss G: 11.2309\n",
            "Epoch 32/49, Batch 300/723, Loss D: 0.5107, Loss G: 11.8375\n",
            "Epoch 32/49, Batch 400/723, Loss D: 0.7194, Loss G: 11.3970\n",
            "Epoch 32/49, Batch 500/723, Loss D: 0.5164, Loss G: 14.0801\n",
            "Epoch 32/49, Batch 600/723, Loss D: 0.5054, Loss G: 11.8901\n",
            "Epoch 32/49, Batch 700/723, Loss D: 0.5777, Loss G: 12.5366\n",
            "Checkpoint saved at epoch 32\n",
            "Epoch 33/49, Batch 0/723, Loss D: 0.7016, Loss G: 9.2545\n",
            "Epoch 33/49, Batch 100/723, Loss D: 0.5234, Loss G: 11.4590\n",
            "Epoch 33/49, Batch 200/723, Loss D: 0.5755, Loss G: 10.4073\n",
            "Epoch 33/49, Batch 300/723, Loss D: 0.4265, Loss G: 10.7844\n",
            "Epoch 33/49, Batch 400/723, Loss D: 0.5461, Loss G: 12.0132\n",
            "Epoch 33/49, Batch 500/723, Loss D: 0.4610, Loss G: 12.5972\n",
            "Epoch 33/49, Batch 600/723, Loss D: 0.5979, Loss G: 11.2583\n",
            "Epoch 33/49, Batch 700/723, Loss D: 0.4341, Loss G: 11.6771\n",
            "Checkpoint saved at epoch 33\n",
            "Epoch 34/49, Batch 0/723, Loss D: 0.4360, Loss G: 11.0958\n",
            "Epoch 34/49, Batch 100/723, Loss D: 0.4493, Loss G: 12.3927\n",
            "Epoch 34/49, Batch 200/723, Loss D: 0.3816, Loss G: 13.4211\n",
            "Epoch 34/49, Batch 300/723, Loss D: 0.5318, Loss G: 12.0449\n",
            "Epoch 34/49, Batch 400/723, Loss D: 0.5012, Loss G: 11.3202\n",
            "Epoch 34/49, Batch 500/723, Loss D: 0.4553, Loss G: 10.8700\n",
            "Epoch 34/49, Batch 600/723, Loss D: 0.5208, Loss G: 11.7677\n",
            "Epoch 34/49, Batch 700/723, Loss D: 0.4729, Loss G: 11.4144\n",
            "Checkpoint saved at epoch 34\n",
            "Epoch 35/49, Batch 0/723, Loss D: 1.3814, Loss G: 8.8658\n",
            "Epoch 35/49, Batch 100/723, Loss D: 0.4913, Loss G: 13.3607\n",
            "Epoch 35/49, Batch 200/723, Loss D: 0.4275, Loss G: 12.6896\n",
            "Epoch 35/49, Batch 300/723, Loss D: 0.5046, Loss G: 11.3863\n",
            "Epoch 35/49, Batch 400/723, Loss D: 0.5444, Loss G: 13.4579\n",
            "Epoch 35/49, Batch 500/723, Loss D: 0.5042, Loss G: 11.8110\n",
            "Epoch 35/49, Batch 600/723, Loss D: 0.4321, Loss G: 14.1197\n",
            "Epoch 35/49, Batch 700/723, Loss D: 0.5146, Loss G: 10.4165\n",
            "Checkpoint saved at epoch 35\n",
            "Epoch 36/49, Batch 0/723, Loss D: 1.5698, Loss G: 14.3689\n",
            "Epoch 36/49, Batch 100/723, Loss D: 0.4366, Loss G: 12.3417\n",
            "Epoch 36/49, Batch 200/723, Loss D: 0.4057, Loss G: 12.4423\n",
            "Epoch 36/49, Batch 300/723, Loss D: 0.4937, Loss G: 12.6409\n",
            "Epoch 36/49, Batch 400/723, Loss D: 0.4826, Loss G: 11.6803\n",
            "Epoch 36/49, Batch 500/723, Loss D: 0.4291, Loss G: 11.8416\n",
            "Epoch 36/49, Batch 600/723, Loss D: 0.4617, Loss G: 11.2103\n",
            "Epoch 36/49, Batch 700/723, Loss D: 0.4370, Loss G: 12.5247\n",
            "Checkpoint saved at epoch 36\n",
            "Epoch 37/49, Batch 0/723, Loss D: 0.4334, Loss G: 11.9851\n",
            "Epoch 37/49, Batch 100/723, Loss D: 0.5879, Loss G: 10.2175\n",
            "Epoch 37/49, Batch 200/723, Loss D: 0.5734, Loss G: 14.3283\n",
            "Epoch 37/49, Batch 300/723, Loss D: 0.4558, Loss G: 11.4450\n",
            "Epoch 37/49, Batch 400/723, Loss D: 0.4899, Loss G: 11.5702\n",
            "Epoch 37/49, Batch 500/723, Loss D: 0.4991, Loss G: 13.4725\n",
            "Epoch 37/49, Batch 600/723, Loss D: 0.4483, Loss G: 13.6460\n",
            "Epoch 37/49, Batch 700/723, Loss D: 0.4834, Loss G: 13.6580\n",
            "Checkpoint saved at epoch 37\n",
            "Epoch 38/49, Batch 0/723, Loss D: 0.5190, Loss G: 9.8800\n",
            "Epoch 38/49, Batch 100/723, Loss D: 0.4613, Loss G: 13.9452\n",
            "Epoch 38/49, Batch 200/723, Loss D: 0.5001, Loss G: 12.6838\n",
            "Epoch 38/49, Batch 300/723, Loss D: 0.5045, Loss G: 11.3042\n",
            "Epoch 38/49, Batch 400/723, Loss D: 0.4264, Loss G: 11.9225\n",
            "Epoch 38/49, Batch 500/723, Loss D: 0.4853, Loss G: 10.6498\n",
            "Epoch 38/49, Batch 600/723, Loss D: 0.6876, Loss G: 11.7015\n",
            "Epoch 38/49, Batch 700/723, Loss D: 0.5805, Loss G: 11.0755\n",
            "Checkpoint saved at epoch 38\n",
            "Epoch 39/49, Batch 0/723, Loss D: 0.6046, Loss G: 12.1610\n",
            "Epoch 39/49, Batch 100/723, Loss D: 0.4567, Loss G: 13.3709\n",
            "Epoch 39/49, Batch 200/723, Loss D: 0.4363, Loss G: 11.2048\n",
            "Epoch 39/49, Batch 300/723, Loss D: 0.5355, Loss G: 10.8986\n",
            "Epoch 39/49, Batch 400/723, Loss D: 0.4835, Loss G: 12.7144\n",
            "Epoch 39/49, Batch 500/723, Loss D: 0.4372, Loss G: 13.9469\n",
            "Epoch 39/49, Batch 600/723, Loss D: 0.4466, Loss G: 12.0478\n",
            "Epoch 39/49, Batch 700/723, Loss D: 0.5471, Loss G: 11.8231\n",
            "Checkpoint saved at epoch 39\n",
            "Epoch 40/49, Batch 0/723, Loss D: 0.4246, Loss G: 12.4809\n",
            "Epoch 40/49, Batch 100/723, Loss D: 0.4792, Loss G: 13.4157\n",
            "Epoch 40/49, Batch 200/723, Loss D: 0.4976, Loss G: 12.4109\n",
            "Epoch 40/49, Batch 300/723, Loss D: 0.5099, Loss G: 10.7606\n",
            "Epoch 40/49, Batch 400/723, Loss D: 0.4384, Loss G: 14.2408\n",
            "Epoch 40/49, Batch 500/723, Loss D: 0.4085, Loss G: 15.4930\n",
            "Epoch 40/49, Batch 600/723, Loss D: 0.4368, Loss G: 11.1600\n",
            "Epoch 40/49, Batch 700/723, Loss D: 0.4625, Loss G: 11.9458\n",
            "Checkpoint saved at epoch 40\n",
            "Epoch 41/49, Batch 0/723, Loss D: 0.4389, Loss G: 12.7453\n",
            "Epoch 41/49, Batch 100/723, Loss D: 0.4321, Loss G: 13.6156\n",
            "Epoch 41/49, Batch 200/723, Loss D: 0.5713, Loss G: 12.4445\n",
            "Epoch 41/49, Batch 300/723, Loss D: 0.4086, Loss G: 13.1955\n",
            "Epoch 41/49, Batch 400/723, Loss D: 0.3634, Loss G: 15.1562\n",
            "Epoch 41/49, Batch 500/723, Loss D: 0.3648, Loss G: 19.6441\n",
            "Epoch 41/49, Batch 600/723, Loss D: 0.3985, Loss G: 18.8301\n",
            "Epoch 41/49, Batch 700/723, Loss D: 0.4761, Loss G: 10.8789\n",
            "Checkpoint saved at epoch 41\n",
            "Epoch 42/49, Batch 0/723, Loss D: 0.4320, Loss G: 12.4145\n",
            "Epoch 42/49, Batch 100/723, Loss D: 0.3969, Loss G: 15.1445\n",
            "Epoch 42/49, Batch 200/723, Loss D: 0.3825, Loss G: 21.8094\n",
            "Epoch 42/49, Batch 300/723, Loss D: 0.3831, Loss G: 17.1426\n",
            "Epoch 42/49, Batch 400/723, Loss D: 0.3899, Loss G: 13.0300\n",
            "Epoch 42/49, Batch 500/723, Loss D: 0.4579, Loss G: 11.2632\n",
            "Epoch 42/49, Batch 600/723, Loss D: 0.4159, Loss G: 10.3811\n",
            "Epoch 42/49, Batch 700/723, Loss D: 0.4186, Loss G: 13.9166\n",
            "Checkpoint saved at epoch 42\n",
            "Epoch 43/49, Batch 0/723, Loss D: 0.4186, Loss G: 11.0823\n",
            "Epoch 43/49, Batch 100/723, Loss D: 0.4127, Loss G: 13.9538\n",
            "Epoch 43/49, Batch 200/723, Loss D: 0.4277, Loss G: 13.8308\n",
            "Epoch 43/49, Batch 300/723, Loss D: 0.4201, Loss G: 11.0330\n",
            "Epoch 43/49, Batch 400/723, Loss D: 0.4476, Loss G: 11.2279\n",
            "Epoch 43/49, Batch 500/723, Loss D: 0.4137, Loss G: 15.0877\n",
            "Epoch 43/49, Batch 600/723, Loss D: 0.4756, Loss G: 12.6658\n",
            "Epoch 43/49, Batch 700/723, Loss D: 0.4346, Loss G: 12.5931\n",
            "Checkpoint saved at epoch 43\n",
            "Epoch 44/49, Batch 0/723, Loss D: 0.3969, Loss G: 14.5385\n",
            "Epoch 44/49, Batch 100/723, Loss D: 0.3950, Loss G: 12.4638\n",
            "Epoch 44/49, Batch 200/723, Loss D: 0.4247, Loss G: 10.9944\n",
            "Epoch 44/49, Batch 300/723, Loss D: 0.3977, Loss G: 13.2178\n",
            "Epoch 44/49, Batch 400/723, Loss D: 0.4044, Loss G: 11.6020\n",
            "Epoch 44/49, Batch 500/723, Loss D: 0.3992, Loss G: 11.1508\n",
            "Epoch 44/49, Batch 600/723, Loss D: 0.4045, Loss G: 13.4636\n",
            "Epoch 44/49, Batch 700/723, Loss D: 0.4170, Loss G: 14.1452\n",
            "Checkpoint saved at epoch 44\n",
            "Epoch 45/49, Batch 0/723, Loss D: 0.3933, Loss G: 11.8067\n",
            "Epoch 45/49, Batch 100/723, Loss D: 0.4159, Loss G: 11.2757\n",
            "Epoch 45/49, Batch 200/723, Loss D: 0.4369, Loss G: 12.4832\n",
            "Epoch 45/49, Batch 300/723, Loss D: 0.4026, Loss G: 13.7164\n",
            "Epoch 45/49, Batch 400/723, Loss D: 0.5362, Loss G: 9.5842\n",
            "Epoch 45/49, Batch 500/723, Loss D: 0.4010, Loss G: 13.0518\n",
            "Epoch 45/49, Batch 600/723, Loss D: 0.4767, Loss G: 10.9772\n",
            "Epoch 45/49, Batch 700/723, Loss D: 0.4048, Loss G: 14.0447\n",
            "Checkpoint saved at epoch 45\n",
            "Epoch 46/49, Batch 0/723, Loss D: 0.3699, Loss G: 15.4138\n",
            "Epoch 46/49, Batch 100/723, Loss D: 0.3994, Loss G: 13.0117\n",
            "Epoch 46/49, Batch 200/723, Loss D: 0.4735, Loss G: 12.3592\n",
            "Epoch 46/49, Batch 300/723, Loss D: 0.4029, Loss G: 14.5258\n",
            "Epoch 46/49, Batch 400/723, Loss D: 0.4402, Loss G: 11.8513\n",
            "Epoch 46/49, Batch 500/723, Loss D: 0.5246, Loss G: 13.3235\n",
            "Epoch 46/49, Batch 600/723, Loss D: 0.4386, Loss G: 12.3671\n",
            "Epoch 46/49, Batch 700/723, Loss D: 0.3920, Loss G: 10.1388\n",
            "Checkpoint saved at epoch 46\n",
            "Epoch 47/49, Batch 0/723, Loss D: 0.5734, Loss G: 14.1014\n",
            "Epoch 47/49, Batch 100/723, Loss D: 0.6499, Loss G: 12.3920\n",
            "Epoch 47/49, Batch 200/723, Loss D: 0.3846, Loss G: 13.2980\n",
            "Epoch 47/49, Batch 300/723, Loss D: 0.5692, Loss G: 11.8581\n",
            "Epoch 47/49, Batch 400/723, Loss D: 0.4309, Loss G: 14.2526\n",
            "Epoch 47/49, Batch 500/723, Loss D: 0.3996, Loss G: 12.3406\n",
            "Epoch 47/49, Batch 600/723, Loss D: 0.4802, Loss G: 10.3713\n",
            "Epoch 47/49, Batch 700/723, Loss D: 0.4668, Loss G: 13.6052\n",
            "Checkpoint saved at epoch 47\n",
            "Epoch 48/49, Batch 0/723, Loss D: 0.4682, Loss G: 9.9488\n",
            "Epoch 48/49, Batch 100/723, Loss D: 0.4365, Loss G: 11.3792\n",
            "Epoch 48/49, Batch 200/723, Loss D: 0.3962, Loss G: 12.2816\n",
            "Epoch 48/49, Batch 300/723, Loss D: 0.4257, Loss G: 12.9329\n",
            "Epoch 48/49, Batch 400/723, Loss D: 0.4188, Loss G: 12.3579\n",
            "Epoch 48/49, Batch 500/723, Loss D: 0.4040, Loss G: 13.7937\n",
            "Epoch 48/49, Batch 600/723, Loss D: 0.4706, Loss G: 13.1402\n",
            "Epoch 48/49, Batch 700/723, Loss D: 0.6870, Loss G: 11.1058\n",
            "Checkpoint saved at epoch 48\n",
            "Epoch 49/49, Batch 0/723, Loss D: 0.4448, Loss G: 11.7363\n",
            "Epoch 49/49, Batch 100/723, Loss D: 0.4236, Loss G: 10.4662\n",
            "Epoch 49/49, Batch 200/723, Loss D: 0.4381, Loss G: 15.3670\n",
            "Epoch 49/49, Batch 300/723, Loss D: 0.4582, Loss G: 12.1192\n",
            "Epoch 49/49, Batch 400/723, Loss D: 0.3883, Loss G: 12.9608\n",
            "Epoch 49/49, Batch 500/723, Loss D: 0.4379, Loss G: 15.5561\n",
            "Epoch 49/49, Batch 600/723, Loss D: 0.4043, Loss G: 13.5457\n",
            "Epoch 49/49, Batch 700/723, Loss D: 0.4087, Loss G: 12.9917\n",
            "Checkpoint saved at epoch 49\n"
          ]
        }
      ],
      "source": [
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Dataset paths (replace with your VoiceBank paths)\n",
        "clean_train_dir = '/content/dataset/clean_trainset_28spk_wav'\n",
        "noisy_train_dir = '/content/dataset/noisy_trainset_28spk_wav'\n",
        "clean_test_dir = '/content/dataset/clean_testset_wav'\n",
        "noisy_test_dir = '/content/dataset/noisy_testset_wav'\n",
        "\n",
        "# Precompute spectrograms to speed up training \n",
        "#precompute_spectrograms(clean_train_dir, noisy_train_dir, PRECOMPUTE_DIR)\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = VoiceBankDataset(clean_train_dir, noisy_train_dir, precompute_dir=PRECOMPUTE_DIR)\n",
        "test_dataset = VoiceBankDataset(clean_test_dir, noisy_test_dir, precompute_dir=PRECOMPUTE_DIR)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Train\n",
        "train(generator, discriminator, train_loader, num_epochs=50, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajtoiKPwJqyp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
